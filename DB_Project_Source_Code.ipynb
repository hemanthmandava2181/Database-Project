{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j3a7hG9tEpZI"
      },
      "outputs": [],
      "source": [
        "# This file is for parsing the input\n",
        "def check_for_comma(column_data):\n",
        "    return column_data.str.contains(',').any()\n",
        "\n",
        "def input_parser(data_file):\n",
        "    data_file = data_file.astype(str)\n",
        "    cols_with_comma = [\n",
        "        column for column in data_file.columns if check_for_comma(data_file[column])]\n",
        "\n",
        "    for column in cols_with_comma:\n",
        "        data_file[column] = data_file[column].str.split(\n",
        "            ',').apply(lambda items: [element.strip() for element in items])\n",
        "\n",
        "    return data_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This file is used for all the normalizations\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import re\n",
        "\n",
        "def is_list_or_set(item):\n",
        "    return isinstance(item, (list, set))\n",
        "\n",
        "\n",
        "def is_superkey(relation, left_hand_side):\n",
        "    grouped = relation.groupby(\n",
        "        list(left_hand_side)).size().reset_index(name='count')\n",
        "    return not any(grouped['count'] > 1)\n",
        "\n",
        "\n",
        "def powerset(s):\n",
        "    x = len(s)\n",
        "    for i in range(1 << x):\n",
        "        yield [s[j] for j in range(x) if (i & (1 << j)) > 0]\n",
        "\n",
        "\n",
        "def bcnf_decomposition(relation, dependencies):\n",
        "    for left_hand_side, right_hand_sides in dependencies.items():\n",
        "        if set(left_hand_side).issubset(relation.columns) and not is_superkey(relation, left_hand_side):\n",
        "            right_hand_side_cols = list(left_hand_side) + right_hand_sides\n",
        "            new_relation1 = relation[right_hand_side_cols].drop_duplicates()\n",
        "            remaining_cols = list(set(relation.columns) - set(right_hand_sides))\n",
        "            new_relation2 = relation[remaining_cols].drop_duplicates()\n",
        "            return [new_relation1, new_relation2]\n",
        "    return [relation]\n",
        "\n",
        "\n",
        "def check_1nf(relation):\n",
        "    if relation.empty:\n",
        "        return False\n",
        "\n",
        "    for column in relation.columns:\n",
        "        unique_types = relation[column].apply(type).nunique()\n",
        "        if unique_types > 1:\n",
        "            return False\n",
        "        if relation[column].apply(lambda x: isinstance(x, (list, dict, set))).any():\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_2nf(primary_key, dependencies):\n",
        "    partial_dependencies_not_found = True\n",
        "    for left_hand_side, right_hand_side in dependencies.items():\n",
        "        if set(left_hand_side).issubset(primary_key) and set(left_hand_side) != set(primary_key):\n",
        "            partial_dependencies_not_found = False\n",
        "            break\n",
        "\n",
        "    return partial_dependencies_not_found\n",
        "\n",
        "\n",
        "def check_3nf(relations, dependencies):\n",
        "    for relation in relations:\n",
        "        attributes = set(relations[relation].columns)\n",
        "        non_prime_attributes = attributes - set(relation)\n",
        "        for left_hand_side, right_hand_sides in dependencies.items():\n",
        "            if all(attr in non_prime_attributes for attr in left_hand_side):\n",
        "                for right_hand_side in right_hand_sides:\n",
        "                    if right_hand_side in non_prime_attributes:\n",
        "                        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_bcnf(relations, primary_key, dependencies):\n",
        "    for relation in relations:\n",
        "        for left_hand_side, right_hand_sides in dependencies.items():\n",
        "            if set(left_hand_side).issubset(relation.columns):\n",
        "                if not is_superkey(relation, left_hand_side):\n",
        "                    return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_4nf(relations, mvd_dependencies):\n",
        "    for relation in relations:\n",
        "        for left_hand_side, right_hand_sides in mvd_dependencies.items():\n",
        "            for right_hand_side in right_hand_sides:\n",
        "                if isinstance(left_hand_side, tuple):\n",
        "                    left_hand_side_cols = list(left_hand_side)\n",
        "                else:\n",
        "                    left_hand_side_cols = [left_hand_side]\n",
        "\n",
        "                if all(col in relation.columns for col in left_hand_side_cols + [right_hand_side]):\n",
        "                    grouped = relation.groupby(left_hand_side_cols)[\n",
        "                        right_hand_side].apply(set).reset_index()\n",
        "                    if len(grouped) < len(relation):\n",
        "                        print(\n",
        "                            f\"Multi-valued dependency violation: {left_hand_side} ->-> {right_hand_side}\")\n",
        "                        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_5nf(relations):\n",
        "    i = 0\n",
        "    candidate_keys_dict = {}\n",
        "    for relation in relations:\n",
        "        print(relation)\n",
        "        user_input = input(\"Enter the candidate keys:\")\n",
        "        print('\\n')\n",
        "        tuples = re.findall(r'\\((.*?)\\)', user_input)\n",
        "        candidate_keys = [tuple(map(str.strip, t.split(','))) for t in tuples]\n",
        "        candidate_keys_dict[i] = candidate_keys\n",
        "        i += 1\n",
        "\n",
        "    print(f'Candidate Keys for tables:')\n",
        "    print(candidate_keys_dict)\n",
        "    print('\\n')\n",
        "\n",
        "    j = 0\n",
        "    for relation in relations:\n",
        "        candidate_keys = candidate_keys_dict[j]\n",
        "        j += 1\n",
        "\n",
        "        data_tuples = [tuple(row) for row in relation.to_numpy()]\n",
        "\n",
        "        def project(data, attributes):\n",
        "            return {tuple(row[attr] for attr in attributes) for row in data}\n",
        "\n",
        "        # Function to check if a set of attributes is a superkey\n",
        "        def is_superkey(attributes):\n",
        "            for key in candidate_keys:\n",
        "                if set(key).issubset(attributes):\n",
        "                    return True\n",
        "            return False, candidate_keys_dict\n",
        "\n",
        "        for i in range(1, len(relation.columns)):\n",
        "            for attrs in combinations(relation.columns, i):\n",
        "                if is_superkey(attrs):\n",
        "                    continue\n",
        "\n",
        "                projected_data = project(data_tuples, attrs)\n",
        "                complement_attrs = set(relation.columns) - set(attrs)\n",
        "                complement_data = project(data_tuples, complement_attrs)\n",
        "\n",
        "                joined_data = {(row1 + row2)\n",
        "                               for row1 in projected_data for row2 in complement_data}\n",
        "                if set(data_tuples) != joined_data:\n",
        "                    print(\"Failed 5NF check for attributes:\", attrs)\n",
        "                    return False, candidate_keys_dict\n",
        "\n",
        "    return True, candidate_keys_dict\n",
        "\n",
        "\n",
        "def validate_first_nf(relation):\n",
        "    flag_1nf = check_1nf(relation)\n",
        "\n",
        "    if flag_1nf:\n",
        "        return relation, flag_1nf\n",
        "    else:\n",
        "        for col in relation.columns:\n",
        "            if relation[col].apply(is_list_or_set).any():\n",
        "                relation = relation.explode(col)\n",
        "\n",
        "        print('Tables after 1NF decomposition:')\n",
        "        print(relation)\n",
        "        print('\\n')\n",
        "        return relation, flag_1nf\n",
        "\n",
        "\n",
        "def validate_second_nf(relation, primary_key, dependencies):\n",
        "    relations = {}\n",
        "    original_relation = relation\n",
        "    flag_2nf = check_2nf(primary_key, dependencies)\n",
        "\n",
        "    if flag_2nf:\n",
        "        relations[primary_key] = relation\n",
        "        return relations, flag_2nf\n",
        "    else:\n",
        "        print('Tables after 2NF decomposition:')\n",
        "        for left_hand_side, right_hand_side in dependencies.items():\n",
        "            cols = list(left_hand_side) + right_hand_side\n",
        "            relations[tuple(left_hand_side)] = relation[cols].drop_duplicates(\n",
        "            ).reset_index(drop=True)\n",
        "            print(relations[left_hand_side])\n",
        "            print('\\n')\n",
        "\n",
        "        junction_cols = []\n",
        "        relation_name = ''\n",
        "        for relation in relations:\n",
        "            if set(relation).issubset(primary_key):\n",
        "                relation_name += \"_\".join(relation)\n",
        "                junction_cols.append(relation)\n",
        "\n",
        "        if len(junction_cols) > 1:\n",
        "            jun_cols = list(junction_cols)\n",
        "            cols = [element for tup in jun_cols for element in tup]\n",
        "            temp_df = original_relation[cols].drop_duplicates(\n",
        "            ).reset_index(drop=True)\n",
        "\n",
        "            renamed_cols = [col + '_fk' for col in cols]\n",
        "            temp_df.columns = renamed_cols + \\\n",
        "                [col for col in temp_df.columns if col not in cols]\n",
        "\n",
        "            temp_df[relation_name] = range(1, len(temp_df) + 1)\n",
        "            columns_order = [relation_name] + renamed_cols\n",
        "            temp_df = temp_df[columns_order]\n",
        "            relations[relation_name] = temp_df\n",
        "            print(relations[relation_name])\n",
        "            print('\\n')\n",
        "\n",
        "        return relations, flag_2nf\n",
        "\n",
        "\n",
        "def validate_third_nf(relations, primary_key, dependencies):\n",
        "    three_relations = {}\n",
        "    flag_3nf = check_3nf(relations, dependencies)\n",
        "\n",
        "    if flag_3nf:\n",
        "        return relations, flag_3nf\n",
        "    else:\n",
        "        print('Tables after 3NF decomposition:')\n",
        "        for relation in relations:\n",
        "            original_relation = relations[relation]\n",
        "            for left_hand_side, right_hand_side in dependencies.items():\n",
        "                cols = list(left_hand_side) + right_hand_side\n",
        "                three_relations[tuple(left_hand_side)] = relations[relation][cols].drop_duplicates(\n",
        "                ).reset_index(drop=True)\n",
        "                print(three_relations[left_hand_side])\n",
        "                print('\\n')\n",
        "\n",
        "        junction_cols = []\n",
        "        relation_name = ''\n",
        "        for relation in three_relations:\n",
        "            relation_name += \"_\".join(relation)\n",
        "            junction_cols.append(relation)\n",
        "\n",
        "        print(relation_name)\n",
        "\n",
        "        if len(junction_cols) > 1:\n",
        "            jun_cols = list(junction_cols)\n",
        "            cols = [element for tup in jun_cols for element in tup]\n",
        "            temp_df = original_relation[cols].drop_duplicates(\n",
        "            ).reset_index(drop=True)\n",
        "\n",
        "            renamed_cols = [col + '_fk' for col in cols]\n",
        "            temp_df.columns = renamed_cols + \\\n",
        "                [col for col in temp_df.columns if col not in cols]\n",
        "\n",
        "            temp_df[relation_name] = range(1, len(temp_df) + 1)\n",
        "            columns_order = [relation_name] + renamed_cols\n",
        "            temp_df = temp_df[columns_order]\n",
        "            three_relations[relation_name] = temp_df\n",
        "            print(three_relations[relation_name])\n",
        "            print('\\n')\n",
        "\n",
        "        return three_relations, flag_3nf\n",
        "\n",
        "\n",
        "def validate_bc_nf(relations, primary_key, dependencies):\n",
        "    relations = list(relations.values())\n",
        "    bcnf_relations = []\n",
        "    flag_bcnf = check_bcnf(relations, primary_key, dependencies)\n",
        "\n",
        "    if flag_bcnf:\n",
        "        return relations, flag_bcnf\n",
        "    else:\n",
        "        print('Tables after BCNF decomposition:')\n",
        "        for relation in relations:\n",
        "            bcnf_decomposed_relation = bcnf_decomposition(\n",
        "                relation, dependencies)\n",
        "            if len(bcnf_decomposed_relation) == 1:\n",
        "                bcnf_relations.append(bcnf_decomposed_relation)\n",
        "            else:\n",
        "                relations.extend(bcnf_decomposed_relation)\n",
        "\n",
        "    return bcnf_relations, flag_bcnf\n",
        "\n",
        "\n",
        "def validate_fourth_nf(relations, mvd_dependencies):\n",
        "    four_relations = []\n",
        "    flag_4nf = check_4nf(relations, mvd_dependencies)\n",
        "\n",
        "    if flag_4nf:\n",
        "        return relations, flag_4nf\n",
        "    else:\n",
        "        print('Tables after 4NF decomposition:')\n",
        "        for relation in relations:\n",
        "            for left_hand_side, right_hand_sides in mvd_dependencies.items():\n",
        "                for right_hand_side in right_hand_sides:\n",
        "                    if isinstance(left_hand_side, tuple):\n",
        "                        left_hand_side_cols = list(left_hand_side)\n",
        "                    else:\n",
        "                        left_hand_side_cols = [left_hand_side]\n",
        "\n",
        "                    if all(col in relation.columns for col in left_hand_side_cols + [right_hand_side]):\n",
        "                        # Check for multi-valued dependency\n",
        "                        grouped = relation.groupby(left_hand_side_cols)[\n",
        "                            right_hand_side].apply(set).reset_index()\n",
        "                        if len(grouped) < len(relation):\n",
        "                            table_1 = relation[left_hand_side_cols +\n",
        "                                               [right_hand_side]].drop_duplicates()\n",
        "                            table_2 = relation[left_hand_side_cols + [col for col in relation.columns if col not in [\n",
        "                                right_hand_side] + left_hand_side_cols]].drop_duplicates()\n",
        "\n",
        "                            four_relations.extend([table_1, table_2])\n",
        "\n",
        "                            break\n",
        "                else:\n",
        "                    continue\n",
        "                break\n",
        "            else:\n",
        "                four_relations.append(relation)\n",
        "\n",
        "    if len(four_relations) == len(relations):\n",
        "        return four_relations\n",
        "    else:\n",
        "        return validate_fourth_nf(four_relations, mvd_dependencies)\n",
        "\n",
        "\n",
        "def decompose_5nf(dataframe, candidate_keys):\n",
        "    def project(df, attributes):\n",
        "        return df[list(attributes)].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # Function to check if a decomposition is lossless\n",
        "    def is_lossless(df, df1, df2):\n",
        "        common_columns = set(df1.columns) & set(df2.columns)\n",
        "        if not common_columns:\n",
        "            return False\n",
        "        joined_df = pd.merge(df1, df2, how='inner', on=list(common_columns))\n",
        "        return df.equals(joined_df)\n",
        "\n",
        "    decomposed_tables = [dataframe]\n",
        "\n",
        "    for key in candidate_keys:\n",
        "        new_tables = []\n",
        "        for table in decomposed_tables:\n",
        "            if set(key).issubset(set(table.columns)):\n",
        "                table1 = project(table, key)\n",
        "                remaining_columns = set(table.columns) - set(key)\n",
        "                table2 = project(table, remaining_columns | set(key))\n",
        "\n",
        "                if is_lossless(table, table1, table2):\n",
        "                    new_tables.extend([table1, table2])\n",
        "                else:\n",
        "                    new_tables.append(table)\n",
        "            else:\n",
        "                new_tables.append(table)\n",
        "        decomposed_tables = new_tables\n",
        "\n",
        "    return decomposed_tables\n",
        "\n",
        "\n",
        "def validate_fifth_nf(relations, primary_key, dependencies):\n",
        "    five_relations = []\n",
        "    flag_5nf, candidate_keys_dict = check_5nf(relations)\n",
        "\n",
        "    if flag_5nf:\n",
        "        return relations, flag_5nf\n",
        "    else:\n",
        "        print('Tables after 5NF decomposition:')\n",
        "        i = 0\n",
        "        for relation in relations:\n",
        "            candidate_keys = candidate_keys_dict[i]\n",
        "            i += 1\n",
        "            decomposed_relations = decompose_5nf(relation, candidate_keys)\n",
        "            five_relations.append(decomposed_relations)\n",
        "\n",
        "    return five_relations, flag_5nf\n"
      ],
      "metadata": {
        "id": "rpD6u4vsEsiJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This file is used for output generator\n",
        "def pd2sql(dtype):\n",
        "    \"\"\"Function to convert pandas dtype to SQL data type.\"\"\"\n",
        "    if \"int\" in str(dtype):\n",
        "        return \"INT\"\n",
        "    elif \"float\" in str(dtype):\n",
        "        return \"FLOAT\"\n",
        "    elif \"object\" in str(dtype):\n",
        "        return \"VARCHAR(255)\"\n",
        "    elif \"datetime\" in str(dtype):\n",
        "        return \"DATETIME\"\n",
        "    else:\n",
        "        return \"TEXT\"\n",
        "\n",
        "\n",
        "def generate_1nf(primary_keys, df):\n",
        "    table_name = \"_\".join(primary_keys) + \"_table\"\n",
        "    # Start creating the SQL query\n",
        "    query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for column, dtype in zip(df.columns, df.dtypes):\n",
        "        if column in primary_keys:\n",
        "            query += f\"  {column} {pd2sql(dtype)} PRIMARY KEY,\\n\"\n",
        "        else:\n",
        "            query += f\"  {column} {pd2sql(dtype)},\\n\"\n",
        "\n",
        "    query = query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    print(query)\n",
        "\n",
        "\n",
        "def generate_2nf_3nf(relations):\n",
        "    for relation in relations:\n",
        "        primary_keys = relation\n",
        "        table_name = \"_\".join(relation) + '_table'\n",
        "        relation = relations[relation]\n",
        "\n",
        "        query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "        for column, dtype in zip(relation.columns, relation.dtypes):\n",
        "            if column in primary_keys:\n",
        "                query += f\"  {column} {pd2sql(dtype)} PRIMARY KEY,\\n\"\n",
        "            else:\n",
        "                query += f\"  {column} {pd2sql(dtype)},\\n\"\n",
        "\n",
        "        query = query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "        print(query)\n",
        "\n",
        "\n",
        "def generate_bcnf_4nf_5nf(relations):\n",
        "    for relation in relations:\n",
        "        primary_key = relation.columns[0]\n",
        "        table_name = f'{primary_key}_table'\n",
        "\n",
        "        query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "        for column, dtype in zip(relation.columns, relation.dtypes):\n",
        "            if column == primary_key:\n",
        "                query += f\"  {column} {pd2sql(dtype)} PRIMARY KEY,\\n\"\n",
        "            else:\n",
        "                query += f\"  {column} {pd2sql(dtype)},\\n\"\n",
        "\n",
        "        query = query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "        print(query)\n"
      ],
      "metadata": {
        "id": "xJ8IATWeEuk9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main file for reading CSV data and performing normalization operations\n",
        "import pandas as pd\n",
        "import csv\n",
        "import normalization_procedures\n",
        "import input_parser\n",
        "from sql_table_creator import generate_1nf, generate_2nf_3nf, generate_bcnf_4nf_5nf\n",
        "\n",
        "# Reading the input csv file and the dependencies text file\n",
        "input_file = pd.read_csv('/content/exampleInputTable.csv')\n",
        "print('Input Relation Tables:')\n",
        "print(input_file)\n",
        "print('\\n')\n",
        "\n",
        "with open('/content/dependency_parser.txt', 'r') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "\n",
        "dependencies = {}\n",
        "for line in lines:\n",
        "    left_hand_side, right_hand_side = line.split(\" -> \")\n",
        "    left_hand_side = left_hand_side.split(\", \")\n",
        "    dependencies[tuple(left_hand_side)] = right_hand_side.split(\", \")\n",
        "print('Dependencies:')\n",
        "print(dependencies)\n",
        "print('\\n')\n",
        "\n",
        "# Input from the user\n",
        "target_normal_form = input(\n",
        "    'Choice of the highest normal form to reach (1: 1NF, 2: 2NF, 3: 3NF, B: BCNF, 4: 4NF, 5: 5NF):')\n",
        "if target_normal_form in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
        "    target_normal_form = int(target_normal_form)\n",
        "\n",
        "# Find the highest normal form of the input relation\n",
        "find_high_nf = int(\n",
        "    input('Find the highest normal form of the input table? (1: Yes, 2: No):'))\n",
        "high_nf = 'No normalization done.'\n",
        "\n",
        "primary_key = input(\n",
        "    \"Enter the Primary Key values: \").split(', ')\n",
        "print('\\n')\n",
        "\n",
        "keys = ()\n",
        "for key in primary_key:\n",
        "    keys = keys + (key,)\n",
        "\n",
        "primary_key = keys\n",
        "\n",
        "mvd_dependencies = {}\n",
        "if not target_normal_form == 'B' and target_normal_form >= 4:\n",
        "    with open('/content/mvd_dependencies.txt', 'r') as file:\n",
        "        mvd_lines = [line.strip() for line in file]\n",
        "\n",
        "    print(mvd_lines)\n",
        "\n",
        "    for mvd in mvd_lines:\n",
        "        left_hand_side, right_hand_side = mvd.split(\" ->-> \")\n",
        "        left_hand_side = left_hand_side.split(\n",
        "            \", \") if \", \" in left_hand_side else [left_hand_side]\n",
        "        left_hand_side_str = str(left_hand_side)\n",
        "        if left_hand_side_str in mvd_dependencies:\n",
        "            mvd_dependencies[left_hand_side_str].append(right_hand_side)\n",
        "        else:\n",
        "            mvd_dependencies[left_hand_side_str] = [right_hand_side]\n",
        "\n",
        "    print('Multi-valued Dependencies')\n",
        "    print(mvd_dependencies)\n",
        "    print('\\n')\n",
        "\n",
        "input_file = input_parser.input_parser(input_file)\n",
        "\n",
        "if target_normal_form == 'B' or target_normal_form >= 1:\n",
        "    first_nf_table, flag_1nf = normalization_procedures.validate_first_nf(\n",
        "        input_file)\n",
        "\n",
        "    if flag_1nf:\n",
        "        high_nf = 'Highest Normal Form is: 1NF'\n",
        "\n",
        "    if target_normal_form == 1:\n",
        "        if flag_1nf:\n",
        "            print('Already Normalized to 1NF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to 1NF:')\n",
        "        print('\\n')\n",
        "        generate_1nf(primary_key, first_nf_table)\n",
        "\n",
        "if target_normal_form == 'B' or target_normal_form >= 2:\n",
        "    second_nf_tables, flag_2nf = normalization_procedures.validate_second_nf(\n",
        "        first_nf_table, primary_key, dependencies)\n",
        "\n",
        "    if flag_1nf and flag_2nf:\n",
        "        high_nf = 'Highest Normal Form is: 2NF'\n",
        "\n",
        "    if target_normal_form == 2:\n",
        "        if flag_2nf and flag_1nf:\n",
        "            print('Already Normalized to 2NF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to 2NF')\n",
        "        print('\\n')\n",
        "        generate_2nf_3nf(second_nf_tables)\n",
        "\n",
        "if target_normal_form == 'B' or target_normal_form >= 3:\n",
        "    third_nf_tables, flag_3nf = normalization_procedures.validate_third_nf(\n",
        "        second_nf_tables, primary_key, dependencies)\n",
        "\n",
        "    if flag_1nf and flag_2nf and flag_3nf:\n",
        "        high_nf = 'Highest Normal Form is: 3NF'\n",
        "\n",
        "    if target_normal_form == 3:\n",
        "        if flag_3nf and flag_2nf and flag_1nf:\n",
        "            print('Already Normalized to 3NF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to 3NF')\n",
        "        print('\\n')\n",
        "        generate_2nf_3nf(third_nf_tables)\n",
        "\n",
        "if target_normal_form == 'B' or target_normal_form >= 4:\n",
        "    bcnf_tables, flag_bcnf = normalization_procedures.validate_bc_nf(\n",
        "        third_nf_tables, primary_key, dependencies)\n",
        "\n",
        "    if flag_1nf and flag_2nf and flag_3nf and flag_bcnf:\n",
        "        high_nf = 'Highest Normal Form is: BCNF'\n",
        "\n",
        "    if target_normal_form == 'B':\n",
        "        if flag_bcnf and flag_3nf and flag_2nf and flag_1nf:\n",
        "            print('Already Normalized to BCNF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to BCNF')\n",
        "        print('\\n')\n",
        "        generate_bcnf_4nf_5nf(bcnf_tables)\n",
        "\n",
        "if not target_normal_form == 'B' and target_normal_form >= 4:\n",
        "    fourth_nf_tables, flag_4nf = normalization_procedures.validate_fourth_nf(\n",
        "        bcnf_tables, mvd_dependencies)\n",
        "\n",
        "    if flag_1nf and flag_2nf and flag_3nf and flag_bcnf and flag_4nf:\n",
        "        high_nf = 'Highest Normal Form is: 4NF'\n",
        "\n",
        "    if target_normal_form == 4:\n",
        "        if flag_4nf and flag_bcnf and flag_3nf and flag_2nf and flag_1nf:\n",
        "            print('Already Normalized to 4NF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to 4NF')\n",
        "        print('\\n')\n",
        "        generate_bcnf_4nf_5nf(fourth_nf_tables)\n",
        "\n",
        "if not target_normal_form == 'B' and target_normal_form >= 5:\n",
        "    fifth_nf_tables, flag_5nf = normalization_procedures.validate_fifth_nf(\n",
        "        fourth_nf_tables, primary_key, dependencies)\n",
        "\n",
        "    if flag_1nf and flag_2nf and flag_3nf and flag_bcnf and flag_4nf and flag_5nf:\n",
        "        high_nf = 'Highest Normal Form is: 5NF'\n",
        "\n",
        "    if target_normal_form == 5:\n",
        "        if flag_5nf and flag_4nf and flag_bcnf and flag_3nf and flag_2nf and flag_1nf:\n",
        "            print('Already Normalized to 5NF')\n",
        "            print('\\n')\n",
        "\n",
        "        print('Queries after decomposing to 5NF')\n",
        "        print('\\n')\n",
        "        generate_bcnf_4nf_5nf(fifth_nf_tables)\n",
        "\n",
        "if find_high_nf == 1:\n",
        "    print('\\n')\n",
        "    print(high_nf)\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exJVKOuXEyRO",
        "outputId": "7b932386-1bc9-41d9-eba1-92f1a63b1eed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Relation Tables:\n",
            "   StudentID FirstName  LastName   Course  Professor  ProfessorEmail  \\\n",
            "0        101      John       Doe  Math101   Dr.Smith   smith@mst.edu   \n",
            "1        102      Jane       Roe  Math101   Dr.Smith   smith@mst.edu   \n",
            "2        103   Arindam    Khanda    CS101   Dr.Jones   jones@mst.edu   \n",
            "3        104      Jose  Franklin   Bio101  Dr.Watson  watson@mst.edu   \n",
            "4        105       Ada  Lovelace    CS101   Dr.Jones   jones@mst.edu   \n",
            "\n",
            "  CourseStart  CourseEnd  \n",
            "0  01-01-2023  5/30/2023  \n",
            "1  01-01-2023  5/30/2023  \n",
            "2  02-01-2023  6/15/2023  \n",
            "3  03-01-2023  7/20/2023  \n",
            "4  02-01-2023  6/15/2023  \n",
            "\n",
            "\n",
            "Dependencies:\n",
            "{('StudentID',): ['FirstName', 'LastName'], ('Course',): ['CourseStart', 'CourseEnd', 'Professor'], ('Professor',): ['ProfessorEmail']}\n",
            "\n",
            "\n",
            "Choice of the highest normal form to reach (1: 1NF, 2: 2NF, 3: 3NF, B: BCNF, 4: 4NF, 5: 5NF):2\n",
            "Find the highest normal form of the input table? (1: Yes, 2: No):1\n",
            "Enter the Primary Key values: StudentID, Course\n",
            "\n",
            "\n",
            "Tables after 2NF decomposition:\n",
            "  StudentID FirstName  LastName\n",
            "0       101      John       Doe\n",
            "1       102      Jane       Roe\n",
            "2       103   Arindam    Khanda\n",
            "3       104      Jose  Franklin\n",
            "4       105       Ada  Lovelace\n",
            "\n",
            "\n",
            "    Course CourseStart  CourseEnd  Professor\n",
            "0  Math101  01-01-2023  5/30/2023   Dr.Smith\n",
            "1    CS101  02-01-2023  6/15/2023   Dr.Jones\n",
            "2   Bio101  03-01-2023  7/20/2023  Dr.Watson\n",
            "\n",
            "\n",
            "   Professor  ProfessorEmail\n",
            "0   Dr.Smith   smith@mst.edu\n",
            "1   Dr.Jones   jones@mst.edu\n",
            "2  Dr.Watson  watson@mst.edu\n",
            "\n",
            "\n",
            "   StudentIDCourse StudentID_fk Course_fk\n",
            "0                1          101   Math101\n",
            "1                2          102   Math101\n",
            "2                3          103     CS101\n",
            "3                4          104    Bio101\n",
            "4                5          105     CS101\n",
            "\n",
            "\n",
            "Queries after decomposing to 2NF\n",
            "\n",
            "\n",
            "CREATE TABLE StudentID_table (\n",
            "  StudentID VARCHAR(255) PRIMARY KEY,\n",
            "  FirstName VARCHAR(255),\n",
            "  LastName VARCHAR(255)\n",
            ");\n",
            "CREATE TABLE Course_table (\n",
            "  Course VARCHAR(255) PRIMARY KEY,\n",
            "  CourseStart VARCHAR(255),\n",
            "  CourseEnd VARCHAR(255),\n",
            "  Professor VARCHAR(255)\n",
            ");\n",
            "CREATE TABLE Professor_table (\n",
            "  Professor VARCHAR(255) PRIMARY KEY,\n",
            "  ProfessorEmail VARCHAR(255)\n",
            ");\n",
            "CREATE TABLE S_t_u_d_e_n_t_I_D_C_o_u_r_s_e_table (\n",
            "  StudentIDCourse INT PRIMARY KEY,\n",
            "  StudentID_fk VARCHAR(255),\n",
            "  Course_fk VARCHAR(255)\n",
            ");\n",
            "\n",
            "\n",
            "Highest Normal Form is: 1NF\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}